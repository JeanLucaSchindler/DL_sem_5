{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MY475 Seminar 5: Language models fundamentals and further applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1) Review of vector and matrix computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pen and paper:\n",
    "\n",
    "a) Compute the dot product between\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "4 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\quad\n",
    "\n",
    "\\text{and}\n",
    "\n",
    "\\quad\n",
    "\n",
    "\\begin{pmatrix}\n",
    "0.4 \\\\\n",
    "0.8 \\\\\n",
    "0.2\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "b) Multiply the matrices\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\quad\n",
    "\n",
    "\\text{and}\n",
    "\n",
    "\\quad\n",
    "\n",
    "\\begin{pmatrix}\n",
    "4 & 3 \\\\\n",
    "2 & 1\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "c) Compute the softmax for the vector\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{pmatrix}\n",
    "4.2 \\\\\n",
    "-3 \\\\\n",
    "0.2\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 1: [0.981 0.001 0.018]\n",
      "T = 0.5: [1. 0. 0.]\n",
      "T = 2: [0.86  0.024 0.116]\n",
      "T = 10: [0.464 0.226 0.311]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_with_temperature(x, T=1.0):\n",
    "    x = x - np.max(x)  # numerical stability (for ex: exp(1000) could not be processed by computer)\n",
    "    x = x / T          # scale by temperature\n",
    "    exps = np.exp(x)\n",
    "    return np.round(exps / np.sum(exps),3)\n",
    "\n",
    "x = np.array([4.2, -3, 0.2])\n",
    "\n",
    "print(\"T = 1:\", softmax_with_temperature(x, T=1))\n",
    "print(\"T = 0.5:\", softmax_with_temperature(x, T=0.5))\n",
    "print(\"T = 2:\", softmax_with_temperature(x, T=2))\n",
    "print(\"T = 10:\", softmax_with_temperature(x, T=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Understanding the Transformer Architecture (with a Focus on Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document explains the transformer architecture and how attention works, using a concrete example with three tokens: `x0`, `x1`, and `x2`. We specifically follow how `x2` flows through a single transformer block.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  The Big Idea: What is a Transformer?\n",
    "\n",
    "A **transformer** is a deep learning architecture designed to process sequences, like sentences, by allowing each element to **interact with all others**â€”regardless of position.\n",
    "\n",
    "Each transformer block consists of two main components:\n",
    "\n",
    "1. **Multi-head self-attention** â€“ lets each token \"look at\" other tokens and decide what is relevant.\n",
    "2. **Feedforward neural network** â€“ a small MLP applied independently to each token.\n",
    "\n",
    "Other essential components:\n",
    "- **Positional encodings** (to encode the order of tokens)\n",
    "- **Residual connections** and **Layer Normalisation** (to stabilize and accelerate training)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ What is Attention?\n",
    "\n",
    "**Self-attention** is the key mechanism in transformers. Each token is mapped to three vectors:\n",
    "- **Query ($Q$)** â€“ what the token is looking for\n",
    "- **Key ($K$)** â€“ what the token offers\n",
    "- **Value ($V$)** â€“ what the token sends if it's attended to\n",
    "\n",
    "Imagine a meeting:\n",
    "- Query = a person's question\n",
    "- Key = their expertise\n",
    "- Value = what they say if asked\n",
    "\n",
    "Each token compares its query with all keys to compute **attention weights**, which it then uses to take a weighted average over all value vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Setup: Input Sequence\n",
    "\n",
    "We consider a sequence with three tokens:\n",
    "- $x_0 = \\text{\"Language\"}$\n",
    "- $x_1 = \\text{\"models\"}$\n",
    "- $x_2 = \\text{\"work\"}$\n",
    "\n",
    "We will now trace how $x_2$ flows through a transformer block.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ›  Step-by-Step: What Happens to $x_2$?\n",
    "\n",
    "### Step 1: Input Embedding\n",
    "\n",
    "We get the embedding vector $e_2$ for token $x_2$, and add its positional encoding $p_2$:\n",
    "\n",
    "$$\n",
    "x_2 = e_2 + p_2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Compute Query, Key, and Value Vectors\n",
    "\n",
    "We apply linear transformations to compute:\n",
    "\n",
    "- Query: $q_2 = W_Q x_2$\n",
    "- Key: $k_2 = W_K x_2$\n",
    "- Value: $v_2 = W_V x_2$\n",
    "\n",
    "We do the same for $x_0$ and $x_1$ to get $q_0$, $k_0$, $v_0$ and $q_1$, $k_1$, $v_1$.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Compute Attention Weights\n",
    "\n",
    "We calculate dot products between $q_2$ and each key to score how much $x_2$ should attend to each token:\n",
    "\n",
    "- $q_2 \\cdot k_0$\n",
    "- $q_2 \\cdot k_1$\n",
    "- $q_2 \\cdot k_2$\n",
    "\n",
    "Then apply the softmax with scaling:\n",
    "\n",
    "$$\n",
    "\\alpha_{2j} = \\frac{\\exp\\left(q_2 \\cdot k_j / \\sqrt{d}\\right)}{\\sum_{t=0}^{2} \\exp\\left(q_2 \\cdot k_t / \\sqrt{d}\\right)}\n",
    "$$\n",
    "\n",
    "Where $d$ is the dimension of the key and query vectors.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Aggregate Values\n",
    "\n",
    "Now $x_2$ builds its new representation by combining value vectors:\n",
    "\n",
    "$$\n",
    "x'_2 = \\alpha_{2,0} v_0 + \\alpha_{2,1} v_1 + \\alpha_{2,2} v_2\n",
    "$$\n",
    "\n",
    "This $x'_2$ now encodes context from the full sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Feedforward Network\n",
    "\n",
    "We pass $x'_2$ through a feedforward network:\n",
    "\n",
    "1. First layer: apply GELU activation  \n",
    "   $$h = \\text{GELU}(W_1 x'_2 + b_1)$$\n",
    "\n",
    "2. Output layer:  \n",
    "   $$x''_2 = W_2 h + b_2$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Residual Connections & Normalisation\n",
    "\n",
    "We apply residual connections and layer normalisation to stabilize training:\n",
    "\n",
    "1. After attention:  \n",
    "   $$\n",
    "   x'_2 = \\text{LayerNorm}(x_2 + \\text{SelfAttention}(x_2))\n",
    "   $$\n",
    "\n",
    "2. After feedforward block:  \n",
    "   $$\n",
    "   x''_2 = \\text{LayerNorm}(x'_2 + \\text{FFN}(x'_2))\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“£ Final Output for $x_2$\n",
    "\n",
    "After passing through the block:\n",
    "- $x''_2$ is the updated, contextualised embedding for token 2.\n",
    "- It's informed by other tokens and ready for the next transformer block.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Summary Visual (Simplified)\n",
    "\n",
    "```plaintext\n",
    "Input:      x0        x1        x2\n",
    "             â†“         â†“         â†“\n",
    "           Embeddings + Positional Encoding\n",
    "             â†“         â†“         â†“\n",
    "          Self-Attention (x2 attends to x0, x1, x2)\n",
    "             â†“         â†“         â†“\n",
    "           New x2' = weighted sum of v0, v1, v2\n",
    "             â†“\n",
    "         Feedforward + Residual + Norm\n",
    "             â†“\n",
    "         Output x2''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer Architecture Diagram](Transformer-neural-network-12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 1000        # number of unique tokens\n",
    "embed_dim = 128          # size of token and position embeddings\n",
    "context_length = 100     # number of positions\n",
    "\n",
    "# Embedding matrices\n",
    "Winputtokemb = np.random.randn(embed_dim, vocab_size)       # (128, 1000)\n",
    "Winputposemb = np.random.randn(embed_dim, context_length)   # (128, 100)\n",
    "\n",
    "# Attention weight matrices\n",
    "WQ = np.random.randn(embed_dim, embed_dim)\n",
    "WK = np.random.randn(embed_dim, embed_dim)\n",
    "WV = np.random.randn(embed_dim, embed_dim)\n",
    "\n",
    "# Feedforward weights\n",
    "W1ff = np.random.randn(256, embed_dim)\n",
    "W2ff = np.random.randn(embed_dim, 256)\n",
    "b1ff = np.random.randn(256)\n",
    "b2ff = np.random.randn(embed_dim)\n",
    "\n",
    "# Final projection (logits) layer\n",
    "Wlinear = np.random.randn(vocab_size, embed_dim)\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # for numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Input tokens (indices into vocabulary) ====\n",
    "X = Winputtokemb[:, [17, 500, 4]] + Winputposemb[:, [0,1,2]]\n",
    "\n",
    "\n",
    "#                   =================  TRANSFORMER BLOCK =================\n",
    "\n",
    "# === Self-Attention for x2 ===\n",
    "# Compute query for x2\n",
    "x2Q = WQ @ X[:, 2]  # shape: (128,)\n",
    "\n",
    "# Compute keys and values for all tokens\n",
    "K = WK @ X          # shape: (128, 3)\n",
    "V = WV @ X          # shape: (128, 3)\n",
    "\n",
    "# Compute attention scores: dot product of q2 with each key\n",
    "scores = x2Q @ K    # shape: (3,) â€” one score per key (x0, x1, x2)\n",
    "scores = scores / np.sqrt(embed_dim)  # scale by sqrt(d)\n",
    "alpha = softmax(scores)               # shape: (3,) â€” attention weights\n",
    "\n",
    "# Weighted sum of values\n",
    "x2_prime = V @ alpha                  # shape: (128,) â€” new representation for x2\n",
    "x2_mid = X[:, 2] + x2_prime\n",
    "\n",
    "# === Feedforward Network ===\n",
    "h = relu(W1ff @ x2_mid + b1ff)     # shape: (256,)\n",
    "x_ff_out = W2ff @ h + b2ff           # shape: (128,)\n",
    "\n",
    "# === Residual Connection ===\n",
    "x2_output = x2_mid + x_ff_out       # shape: (128,)\n",
    "\n",
    "\n",
    "## HERE, WE SHOULD HAVE ALSO NORMALIZED AFTER ADDING THE RESIDUAL CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Project into vocabulary space\n",
    "logits = Wlinear @ x2_output  # shape: (1000,)\n",
    "\n",
    "# Step 2: Convert to probabilities\n",
    "probs = softmax(logits)       # shape: (1000,)\n",
    "\n",
    "# Step 3: Predict next token\n",
    "next_token_id = np.argmax(probs)  # or use sampling\n",
    "\n",
    "\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Retrieval augment generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconsider the AER abstract dataset from the last seminar. Using the sentence transformer library (https://sbert.net/), can you build a simple retrieval part for a RAG system? These text chunks could then be added to the context of a language model, e.g. via an API or a local model.\n",
    "\n",
    "- Encode all abstracts or titles\n",
    "\n",
    "- Write a function that inputs a user questions, encodes it, takes cosine similarity to all embeddings in the dataset, and returns the most similar K texts\n",
    "\n",
    "- If you want to additionally refine your ranking of only the most similar abstracts with a slower model, have a look at https://sbert.net/examples/sentence_transformer/applications/retrieve_rerank/README.html and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeanluca/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:587: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"aer_sample.csv\",\n",
    "    index_col=\"date\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "column = \"title\"\n",
    "\n",
    "# Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Calculate embeddings\n",
    "data_embeddings = model.encode(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 384)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings dim = 384\n",
    "data_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar(query, data_embeddings, df, top_k=5, model=model):\n",
    "\n",
    "    query_embed = model.encode(query)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(\n",
    "        [query_embed], data_embeddings\n",
    "    ).flatten()\n",
    "    # Sort INDEXES by similarity\n",
    "    sorted_indexes = np.argsort(similarities)[::-1]\n",
    "    # Get top K most similar\n",
    "    top_k_indexes = sorted_indexes[:top_k]\n",
    "    # Get the most similar titles\n",
    "    most_similar = df[column].iloc[top_k_indexes]\n",
    "    # Get the most similar scores\n",
    "    most_similar_scores = similarities[top_k_indexes]\n",
    "    # Create a DataFrame for better visualization\n",
    "    result_df = pd.DataFrame(\n",
    "        {\n",
    "            \"title\": most_similar,\n",
    "            \"similarity\": most_similar_scores,\n",
    "        }\n",
    "    )\n",
    "    result_df.index = df.index[top_k_indexes]\n",
    "    result_df.index.name = \"date\"\n",
    "\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>Immigration Restrictions as Active Labor Marke...</td>\n",
       "      <td>0.573339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-01</th>\n",
       "      <td>The Spending and Debt Response to Minimum Wage...</td>\n",
       "      <td>0.535807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-06-01</th>\n",
       "      <td>Increasing Residual Wage Inequality: Compositi...</td>\n",
       "      <td>0.533642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-03-01</th>\n",
       "      <td>Changes in the Cyclical Sensitivity of Wages i...</td>\n",
       "      <td>0.521311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-12-01</th>\n",
       "      <td>Output Growth, the Real Wage, and Employment F...</td>\n",
       "      <td>0.514387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  similarity\n",
       "date                                                                     \n",
       "2018-06-01  Immigration Restrictions as Active Labor Marke...    0.573339\n",
       "2012-12-01  The Spending and Debt Response to Minimum Wage...    0.535807\n",
       "2006-06-01  Increasing Residual Wage Inequality: Compositi...    0.533642\n",
       "1992-03-01  Changes in the Cyclical Sensitivity of Wages i...    0.521311\n",
       "1991-12-01  Output Growth, the Real Wage, and Employment F...    0.514387"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Does an increase in immigration necessarly lead to a decrease in wages?\"\n",
    "res = find_similar(query, data_embeddings, df)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
