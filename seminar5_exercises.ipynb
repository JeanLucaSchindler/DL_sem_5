{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MY475 Seminar 5: Language models fundamentals and further applications\n",
    "\n",
    "\n",
    "## Review of vector and matrix computations\n",
    "\n",
    "With pen and paper:\n",
    "\n",
    "a) Compute the dot product between\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "4 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\quad\n",
    "\n",
    "\\text{and}\n",
    "\n",
    "\\quad\n",
    "\n",
    "\\begin{pmatrix}\n",
    "0.4 \\\\\n",
    "0.8 \\\\\n",
    "0.2\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "b) Multiply the matrices\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\quad\n",
    "\n",
    "\\text{and}\n",
    "\n",
    "\\quad\n",
    "\n",
    "\\begin{pmatrix}\n",
    "4 & 3 \\\\\n",
    "2 & 1\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "c) Compute the softmax for the vector\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{pmatrix}\n",
    "4.2 \\\\\n",
    "-3 \\\\\n",
    "0.2\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in a language model\n",
    "\n",
    "The following parameters and basic functions correspond to a simple decoder transformer language model with one layer of causal attention and one feed-forward layer.\n",
    "\n",
    "Assume no norms and only a single attention head.\n",
    "\n",
    "a) What is context length this model?\n",
    "\n",
    "b) How many unique tokens does its vocabulary have?\n",
    "\n",
    "c) What is the embeding dimension?\n",
    "\n",
    "d) The user enters the input sequence [17, 500, 4]. Which token has the highest predicted probability to be next?\n",
    "\n",
    "e) Briefly explain in words how you would predict the token after your first prediction (no need to add computations in this part)?\n",
    "\n",
    "To answer this question, write code (using only Numpy) to:\n",
    "\n",
    "- Obtain the embeddings of the input  tokens\n",
    "\n",
    "- Add their positional embeddings\n",
    "\n",
    "- Compute the relevant attention weights and updated embeddings\n",
    "\n",
    "- Transform with the feed-forward neural network component\n",
    "\n",
    "- Map into vocabulary\n",
    "\n",
    "- Apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "Winputtokemb = np.random.randn(128, 1000)\n",
    "Winputposemb = np.random.randn(128, 100)\n",
    "\n",
    "WQ = np.random.randn(128, 128)\n",
    "WK = np.random.randn(128, 128)\n",
    "WV = np.random.randn(128, 128)\n",
    "\n",
    "W1ff = np.random.randn(256, 128)\n",
    "W2ff = np.random.randn(128, 256)\n",
    "b1ff = np.random.randn(256)\n",
    "b2ff = np.random.randn(128)\n",
    "\n",
    "Wlinear = np.random.randn(1000, 128)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Retrieval augment generation (RAG)\n",
    "\n",
    "Reconsider the AER abstract dataset from the last seminar. Using the sentence transformer library (https://sbert.net/), can you build a simple retrieval part for a RAG system? These text chunks could then be added to the context of a language model, e.g. via an API or a local model.\n",
    "\n",
    "- Encode all abstracts or titles\n",
    "\n",
    "- Write a function that inputs a user questions, encodes it, takes cosine similarity to all embeddings in the dataset, and returns the most similar K texts\n",
    "\n",
    "- If you want to additionally refine your ranking of only the most similar abstracts with a slower model, have a look at https://sbert.net/examples/sentence_transformer/applications/retrieve_rerank/README.html and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"data/aer_sample.csv\",\n",
    "    index_col=\"date\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "column = \"title\"\n",
    "\n",
    "# Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Calculate embeddings\n",
    "embeddings = model.encode(df[column])\n",
    "\n",
    "def find_similar(query, embeddings, df, top_k=5):\n",
    "\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: AI led interviews\n",
    "\n",
    "If you have worked with the OpenAI or Claude APIs previously and have a key (no need to register for this course!), you can have a look at setting up the interview platform in https://github.com/friedrichgeiecke/interviews and run it on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlteaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
